
<div class="sright">
  <div class="sub-content-head">
    Maui Scheduler  </div>
  <div id="sub-content-rpt" class="sub-content-rpt" >
 <div class="tab-container docs" id="tab-container">
<div class="topNav">

<div class="docsSearch">
</div>


<div class="navIcons topIcons">
<a href="../index.html"><img src="../home.png" border=0 title="Home" alt="Home"></a>
<a href="../a.acasestudy.html"><img src="../upArrow.png" border=0 title="Up" alt="Up"></a>
<a href="case1.html"><img src="../prevArrow.png" border=0 title="Previous" alt="Previous"></a>
<a href="case3.html"><img src="../nextArrow.png" border=0 title="Next" alt="Next"></a>
</div>
  


<h1>A.2 &nbsp;Case Study: Partitioned Heterogeneous Cluster</h1>

<big><p><b>Overview</b></p></big>
<p>
A site possessing a mixture of uniprocessor and dual
processor nodes desires to dedicate a subset of nodes to time-sharing serial
jobs, a subset to parallel batch jobs, and provide a set of nodes to be
used as overflow.
</p>

<big><p><b>Resources</b></p></big>

<table class="navTable" width=100%>
<tr valign="top">
  <td class="contentLabels">Compute Nodes:</td>
  <td class="contentDetails"><i>Group A</i>: 16 single processor Linux&#0174; nodes with 128 MB RAM and 1 GB
      local scratch space<br><i>Group B</i>: 8 2-way SMP Linux&#0174; nodes with 256
      MB RAM and 4 GB local scratch space<br><i>Group C</i>: 8 single processor
      Linux&#0174; nodes with 192 MB RAM and 2 GB local scratch space</td>
</tr>
<tr valign="top">
  <td class="contentLabels">Resource Manager:</td>
  <td class="contentDetails">OpenPBS 2.3</td>
</tr>
<tr valign="top">
  <td class="contentLabels">Network:</td>
  <td class="contentDetails">100 MB switched Ethernet</td>
</tr>
</table>

<big><p><b>Workload</b></p></big>
<table class="simpleTable">
<tr valign="top">
  <td class="contentLabels">Job Size:</td>
  <td class="contentDetails">Range in size from 1 to 32 processors with approximate quartile job
      frequency distribution of:<br>1 - 2, 3 - 8, 9 - 24, and 25 - 32 nodes</td>
</tr>
<tr valign="top">
  <td class="contentLabels">Job Length:</td>
  <td class="contentDetails">Jobs range in length from 1 to 24 hours</td>
</tr>
<tr valign="top">
  <td nowrap class="contentLabels">Job Owners:</td>
  <td class="contentDetails">Six major groups consisting of about 50 users in total</td>
</tr>
<tr valign="top">
  <td class="contentLabels">Notes:</td>
  <td class="contentDetails">During prime time hours, the majority of jobs submitted are smaller,
      short running development jobs where users are testing out new code and
      new data sets.  The owners of these jobs are often unable to proceed
      with their work until a job they have submitted completes.  Many of
      these jobs are interactive in nature.  Throughout the day, large,
      longer running production workload is also submitted but these jobs do
      not have comparable turnaround time pressure.</td>
</tr>
</table>

<big><p><b>Goals</b></p></big>
<p>
Nodes in Group A must run only parallel jobs.  Nodes in Group B must only
run serial jobs, with up to 4 serial jobs per node.  Nodes in Group C must
not be used unless a job cannot locate resources elsewhere.  The scheduler
should attempt to intelligently load balance the timesharing nodes. 
</p>

<big><p><b>Analysis</b></p></big>
<p>
The network topology is flat and and nodes are homogeneous within each
group.  The easiest way to configure the overflow group is to
create two PBS queues, <i>serial</i> and <i>parallel</i>, with appropriate min
and max node counts as desired. 
</p>
<p>
By default, Moab interprets the PBS <i>exclusive hostlist</i> queue attribute
as constraining jobs in the queue to run only on the nodes contained in the
hostlist.  We can take advantage of this behavior to assign nodes in Group
A and Group C to the queue <i>parallel</i> while the nodes in Group B and Group
C are assigned to the queue <i>serial</i>.  The same can be done with
classes if using Loadleveler.  Moab will incorporate this queue
information when making scheduling decisions. 
</p>
<p>
The next step is to make the scheduler use the overflow nodes of group C
only as a last resort.  This can be accomplished using a negative affinity
<a href="../7.1.3standingreservations.html">standing reservation</a>. 
This configuration will tell the scheduler that these nodes can be used, but
should only be used if it cannot find compute resources elsewhere. 
</p>
<p>
The final step, load balancing, is accomplished in two parts.  First, the
nodes in group B must be configured to allow up to 4 serial jobs to run at a
time.  This is best accomplished using the PBS <i>virtual nodes</i>
capability.  To load balance, simply select the
<a href="../5.2nodeallocation.html">CPULOAD</a> allocation
algorithm in Moab.  This algorithm will instruct Moab to schedule the job
based on which node has the most idle CPU time. 
</p>

<big><p><b>Configuration</b></p></big>
<p>
This site requires both resource manager and scheduler configuration.  The
following Moab configuration would be needed:
</p>
<br>

<p><div class="example">

<pre>
# reserve overflow processors
SRNAME[0]         overflow
SRHOSTLIST[0]     cn0[1-8]          # hostname regular expression
SRCLASSLIST[0]    parallel- batch-  # use minus sign to indicate negative affinity

# allow SMP node sharing
ALLOCATIONPOLICY  CPULOAD
NODEACCESSPOLICY  SHARED
</pre>
</div></p>

<p><div class="example">

<pre>
set queue serial resources_max.nodeccount=1
set queue serial acl_hosts=an01+an02+...an16+cn01+cn02+...cn08
set queue serial acl_host_enable=true

set queue parallel resources_min.nodecount=2
set queue parallel acl_hosts=bn01+bn02+...bn08+cn01+cn02+...cn08
set queue parallel acl_host_enable=true
</pre>
</div></p>

<p><div class="example">

<pre>
bn01 np=4
bn01 np=4
</pre>
</div></p>

<big><p><b>Monitoring</b></p></big>
<p>
The commands <a href="../commands/mdiag.html">mdiag -s</a> and
<a href="../commands/mdiag-reservations.html">mdiag -r</a> can be used to
examine the standing reservations and all reservations in more detail. 
The <a href="../commands/mdiag-class.html">mdiag -c</a> command can be used
to examine and verify class/queue information.  The
<a href="../commands/mshow.html">mshow</a> command may also be used to display
information about a class/queue, job, and reservation. 
</p>

<big><p><b>Conclusions</b></p></big>
<p>
Moab's policy engine supports an incredible range of capabilities. 
Configuring different groups of nodes to provide different types of service is
a straightforward process. 
</p>

<div class="navIcons bottomIcons">
<a href="../index.html"><img src="../home.png" border=0 title="Home" alt="Home"></a>
<a href="../a.acasestudy.html"><img src="../upArrow.png" border=0 title="Up" alt="Up"></a>
<a href="case1.html"><img src="../prevArrow.png" border=0 title="Previous" alt="Previous"></a>
<a href="case3.html"><img src="../nextArrow.png" border=0 title="Next" alt="Next"></a>
</div>
 </div>
 </div>
  </div>
  <div class="sub-content-btm"></div>
</div>
</div>

